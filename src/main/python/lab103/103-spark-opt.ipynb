{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cde72d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.driver_memory = '10G'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fef6a0f-d919-4c87-b582-eacd2b852cee",
   "metadata": {
    "id": "3fef6a0f-d919-4c87-b582-eacd2b852cee"
   },
   "source": [
    "# 103 Spark optimizations\n",
    "\n",
    "The goal of this lab is to understand some of the optimization mechanisms of Spark.\n",
    "\n",
    "- [Spark programming guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "- [RDD APIs](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/RDD.html)\n",
    "- [PairRDD APIs](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/PairRDDFunctions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a037caa76dc389a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:34:50.752064Z",
     "start_time": "2024-10-20T16:34:42.694703Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://BigData:4040\n",
       "SparkContext available as 'sc' (version = 3.5.1, master = local[*], app id = local-1730285310693)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark\n",
       "import org.apache.spark.HashPartitioner\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark\n",
    "import org.apache.spark.HashPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7648dedd-4462-44e4-bcf7-5dc3af6f08a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:35:52.972776Z",
     "start_time": "2024-10-20T16:35:52.357262Z"
    },
    "id": "7648dedd-4462-44e4-bcf7-5dc3af6f08a7",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parseWeather: (row: String)(String, String, String, String, String, Int, Boolean)\n",
       "parseStation: (row: String)(String, String, String, String, String, Double, Double, Double, String, String)\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// WEATHER structure: (usaf,wban,year,month,day,airTemperature,airTemperatureQuality)\n",
    "def parseWeather(row:String) = {\n",
    "    val usaf = row.substring(4,10)\n",
    "    val wban = row.substring(10,15)\n",
    "    val year = row.substring(15,19)\n",
    "    val month = row.substring(19,21)\n",
    "    val day = row.substring(21,23)\n",
    "    val airTemperature = row.substring(87,92)\n",
    "    val airTemperatureQuality = row.charAt(92)\n",
    "\n",
    "    (usaf,wban,year,month,day,airTemperature.toInt/10,airTemperatureQuality == '1')\n",
    "}\n",
    "\n",
    "// STATION structure: (usaf,wban,city,country,state,latitude,longitude,elevation,date_begin,date_end) \n",
    "def parseStation(row:String) = {\n",
    "    def getDouble(str:String) : Double = {\n",
    "        if (str.isEmpty)\n",
    "            return 0\n",
    "        else\n",
    "            return str.toDouble\n",
    "    }\n",
    "    val columns = row.split(\",\").map(_.replaceAll(\"\\\"\",\"\"))\n",
    "    val latitude = getDouble(columns(6))\n",
    "    val longitude = getDouble(columns(7))\n",
    "    val elevation = getDouble(columns(8))\n",
    "    (columns(0),columns(1),columns(2),columns(3),columns(4),latitude,longitude,elevation,columns(9),columns(10))  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c70c02bd-4c8f-4cc2-9a13-544da7c6544d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:35:57.630942Z",
     "start_time": "2024-10-20T16:35:56.554809Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddWeather: org.apache.spark.rdd.RDD[(String, String, String, String, String, Int, Boolean)] = MapPartitionsRDD[2] at map at <console>:30\n",
       "rddStation: org.apache.spark.rdd.RDD[(String, String, String, String, String, Double, Double, Double, String, String)] = MapPartitionsRDD[5] at map at <console>:33\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rddWeather = sc\n",
    "  .textFile(\"../../../../datasets/big/weather-sample10.txt\")\n",
    "  .map(x => parseWeather(x))\n",
    "val rddStation = sc\n",
    "  .textFile(\"../../../../datasets/weather-stations.csv\")\n",
    "  .map(x => parseStation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4b49ee-6852-4025-9e55-3950ff937680",
   "metadata": {
    "id": "ef4b49ee-6852-4025-9e55-3950ff937680"
   },
   "source": [
    "## 103-1 Simple job optimization\n",
    "\n",
    "Optimize the two jobs (avg temperature and max temperature) by avoiding the repetition of the same computations and by enforcing a partitioning criteria.\n",
    "- There are multiple methods to repartition an RDD: check the ```coalesce```, ```partitionBy```, and ```repartition``` methods on the documentation and choose the best one.\n",
    "  - To create a partitioning function, you must ```import org.apache.spark.HashPartitioner``` and then define ```val p = new HashPartitioner(n)``` where ```n``` is the number of partitions to create\n",
    "- Verify your persisted data in the web UI\n",
    "- Verify the execution plan of your RDDs with ```rdd.toDebugString``` (shell only) or on the web UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae20e128-aebc-4340-be2f-9da672fa81f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T15:10:42.682093Z",
     "start_time": "2024-10-20T15:10:41.849647Z"
    },
    "id": "ae20e128-aebc-4340-be2f-9da672fa81f8",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Array[(String, Double)] = Array((10,13.32), (11,8.15), (12,4.08), (01,3.06), (02,5.5), (03,8.31), (04,11.75), (05,15.83), (06,18.53), (07,19.96), (08,20.31), (09,17.24))\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Average temperature for every month\n",
    "rddWeather\n",
    "  .filter(_._6<999)\n",
    "  .map(x => (x._4, x._6))\n",
    "  .aggregateByKey((0.0,0.0))((a,v)=>(a._1+v,a._2+1), (a1,a2)=>(a1._1+a2._1,a1._2+a2._2))\n",
    "  .map({case(k,v)=>(k,Math.round(v._1*100/v._2)/100.0)})\n",
    "  .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b614d5393d1a1c2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T15:11:13.188402Z",
     "start_time": "2024-10-20T15:11:12.853137Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[(String, Int)] = Array((10,55), (11,43), (12,47), (01,55), (02,47), (03,44), (04,48), (05,49), (06,56), (07,56), (08,56), (09,55))\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Maximum temperature for every month\n",
    "rddWeather\n",
    "  .filter(_._6<999)\n",
    "  .map(x => (x._4, x._6))\n",
    "  .reduceByKey((x,y)=>{if(x<y) y else x})\n",
    "  .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2990d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "partitionedRddWeather: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[15] at partitionBy at <console>:29\n",
       "res2: Long = 4185148\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val partitionedRddWeather = rddWeather\n",
    "    .filter(_._6<999)\n",
    "    .map(x => (x._4, x._6))\n",
    "    .partitionBy(new HashPartitioner(12))\n",
    "    .cache()\n",
    "\n",
    "partitionedRddWeather.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fbd9d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Array[(String, Int)] = Array((01,55), (02,47), (03,44), (04,48), (05,49), (06,56), (07,56), (10,55), (11,43), (08,56), (09,55), (12,47))\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partitionedRddWeather\n",
    "    .aggregateByKey((0.0,0.0))((a,v)=>(a._1+v,a._2+1), (a1,a2)=>(a1._1+a2._1,a1._2+a2._2))\n",
    "    .map({case(k,v)=>(k,Math.round(v._1*100/v._2)/100.0)})\n",
    "    .collect()\n",
    "\n",
    "partitionedRddWeather\n",
    "    .reduceByKey((x,y)=>{if(x<y) y else x})\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73d28a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: partitionedRddWeather.type = ShuffledRDD[15] at partitionBy at <console>:29\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partitionedRddWeather.unpersist(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377fbf30-f568-413c-9238-de139db23135",
   "metadata": {
    "id": "377fbf30-f568-413c-9238-de139db23135"
   },
   "source": [
    "## 103-2 RDD preparation\n",
    "\n",
    "Check the five possibilities to prepare the Station RDD for subsequent processing and identify the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16b6b4e-b4b6-4ca3-94bb-11b6c65c03d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T15:51:59.650807Z",
     "start_time": "2024-10-20T15:51:59.467316Z"
    },
    "id": "e16b6b4e-b4b6-4ca3-94bb-11b6c65c03d0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "// _1 and _2 are the fields composing the key; _4 and _8 are country and elevation, respectively\n",
    "/*\n",
    "val rddS1 = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  partitionBy(p2).\n",
    "  cache().\n",
    "  map({case (k,v) => (k,(v._4,v._8))})\n",
    "val rddS2 = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  map({case (k,v) => (k,(v._4,v._8))}).\n",
    "  cache().\n",
    "  partitionBy(p2)\n",
    "val rddS3 = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  partitionBy(p2).\n",
    "  map({case (k,v) => (k,(v._4,v._8))}).\n",
    "  cache()\n",
    "val rddS4 = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  map({case (k,v) => (k,(v._4,v._8))}).\n",
    "  partitionBy(p2).\n",
    "  cache()\n",
    "val rddS5 = rddStation.\n",
    "  map(x => (x._1 + x._2, (x._4,x._8))).\n",
    "  partitionBy(p2).\n",
    "  cache()\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c3071b-c9ee-4c02-a85f-2800b9c4d8ed",
   "metadata": {
    "id": "75c3071b-c9ee-4c02-a85f-2800b9c4d8ed"
   },
   "source": [
    "## 103-3 Joining RDDs\n",
    "\n",
    "Define the join between rddWeather and rddStation and compute:\n",
    "- The maximum temperature for every city\n",
    "- The maximum temperature for every city in the UK: \n",
    "  - ```StationData.country == \"UK\"```\n",
    "- Sort the results by descending temperature\n",
    "  - ```map({case(k,v)=>(v,k)})``` to invert key with value and vice versa\n",
    "\n",
    "Hints & considerations:\n",
    "- Keep only temperature values <999\n",
    "- Join syntax: ```rdd1.join(rdd2)```\n",
    "  - Both RDDs should be structured as key-value RDDs with the same key: usaf + wban\n",
    "- Consider partitioning and caching to optimize the join\n",
    "  - Careful: it is not enough for the two RDDs to have the same number of partitions; they must have the same partitioner!\n",
    "- Verify the execution plan of the join in the web UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48af9252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nPartitions: Int = 12\n",
       "rddS: org.apache.spark.rdd.RDD[(String, (String, String))] = RDD_Stations ShuffledRDD[20] at partitionBy at <console>:32\n",
       "rddW: org.apache.spark.rdd.RDD[(String, Int)] = RDD_Weather ShuffledRDD[23] at partitionBy at <console>:40\n",
       "rddJ: org.apache.spark.rdd.RDD[(String, (Int, String, String))] = ShuffledRDD[28] at partitionBy at <console>:48\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nPartitions = 12\n",
    "\n",
    "// STATION structure: (usaf, wban,city,country,state,latitude,longitude,elevation,date_begin,date_end)\n",
    "val rddS = rddStation\n",
    "  .map(x => (x._1 + x._2, (x._3, x._4)))\n",
    "  .partitionBy(new HashPartitioner(nPartitions))\n",
    "  .setName(\"RDD_Stations\")\n",
    "  .cache()\n",
    "\n",
    "// WEATHER structure: (usaf, wban,year,month,day,airTemperature,airTemperatureQuality)\n",
    "val rddW = rddWeather\n",
    "  .filter(_._6<999)\n",
    "  .map(x => (x._1 + x._2, x._6))\n",
    "  .partitionBy(new HashPartitioner(nPartitions))\n",
    "  .setName(\"RDD_Weather\")\n",
    "  .cache()\n",
    "\n",
    "// JOIN structure: (key, (temperature, city, country))\n",
    "val rddJ = rddW\n",
    "  .join(rddS)\n",
    "  .map({case (key, (temperature, (city, country))) => (key, (temperature, city, country))})\n",
    "  .partitionBy(new HashPartitioner(nPartitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3014f7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddJMaxTemp: org.apache.spark.rdd.RDD[(String, (Int, String, String))] = RDD_Join_MaxTemp MapPartitionsRDD[29] at reduceByKey at <console>:28\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rddJMaxTemp = rddJ\n",
    "    .reduceByKey((x, y) => {if (x._1 > y._1) x else y})\n",
    "    .partitionBy(new HashPartitioner(nPartitions))\n",
    "    .setName(\"RDD_Join_MaxTemp\")\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be5e765e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maxTemperaturePerCity: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[30] at map at <console>:28\n",
       "res5: Array[(String, Int)] = Array((REDWOOD FALLS MUNI AIRPORT,35), (TARO ISLAND,32), (PARNAIBA,32), (NOGLIKI,25), (SALEHARD,27), (ITAITUBA,34), (CUSTER,32), (OUYEN,34), (JINHUA,17), (WENDOVER/AF. AUX. F,37), (MOLOKAI (AMOS),30), (QAISUMAH,47), (NAGASAKI,34), (CEDARA,33), (TRIPOLI INTL,40), (CHIAYI,31), (PRYOR FLD RGNL,36), (BOGUS ROMANIAN,-3), (NEWPORT,20), (ERNEST A. LOVE FIELD ARPT,36), (ASHGABAT,41), (GREENOCK MRCC,24), (TRI CITIES,37), (TANGSHAN,36), (INNSBRUCK,31), (NIAGARA DISTRICT,29), (CENTRE ISLAND,35), (MAASIN,33), (CLAREMORRIS,22), (SAN CLEMENTE ISLAND,26), (MC CHORD FIELD,31), (CAMPO DELL ORO,33), (YARMOUTH,23), (OUARZAZATE,40), (SONNBLICK,9), (SIAULIAI,26), (BODEGA ...\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// max temperature per city\n",
    "val maxTemperaturePerCity = rddJMaxTemp\n",
    "    .map({case (k, (temp, city, country)) => (city, temp)})\n",
    "\n",
    "maxTemperaturePerCity.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f483a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maxTemperaturePerCityInUK: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[32] at map at <console>:29\n",
       "res6: Array[(String, Int)] = Array((GREENOCK MRCC,24), (SALSBURGH,8), (FOYERS,21), (PLATFORM NO. 62101,21), (BLACKPOOL,25), (FAIRFORD,13), (LARKHILL,27), (PLATFORM NO. 63101,15), (ENVIRONM BUOY 62146,14), (BASTREET,20), (MOEL-Y-CRIO,23), (CHIVENOR,24), (ASPATRIA,25), (SELLA NESS,17), (NEWCASTLE,23), (LEEDS BRADFORD,28), (WICK,18), (ENVIRONM BUOY 62128,17), (MARHAM,31), (FIFE NESS,19), (LOCH GLASCARNOCH,21), (SOUTH THULE IS.,8), (SHOREHAM,27), (SOUTHEND,29), (NEWTON,28), (INVERGORDON HARBOUR,20), (LUTON,28), (DUNDRENNAN,21), (CAIRNWELL,17), (LECONFIELD (AUT),24), (REDESDALE CAMP,23), (SHOBDON AIRFIELD,27), (BRISTOL WEA CENTER,27), (BRIZE NORTON,29), (PRESTWICK,25), (FYLINGDALE...\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// max temperature per city in UK\n",
    "val maxTemperaturePerCityInUK = rddJMaxTemp\n",
    "    .filter({case (key, (temp, city, country)) => country == \"UK\"})\n",
    "    .map({case (k, (temp, city, country)) => (city, temp)})\n",
    "\n",
    "maxTemperaturePerCityInUK.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9790ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sortedMaxTempCity: Array[(Int, String)] = Array((56,SPARREVOHN AFS), (56,INDIAN MOUNTAIN AFS), (56,TIN CITY AFS), (56,GALENA A.), (56,POINT LAY), (56,CAPE LISBURNE AFS), (56,TATALINA LRRS), (55,CAPE NEWENHAM AFS), (55,PUERTO CARRENO), (55,AKJOUJT), (55,BAHAWALPUR), (54,CAPE ROMANZOFF AFS), (53,ALI AL SALEM), (52,\"\"), (51,\"\"), (50,LZ BULL / EXERCISE), (50,SUFFOLK EXECUTIVE), (50,AHWAZ), (50,AGHAJARI), (50,SHAHID ASYAEE), (49,INDIANA CO), (49,KUWAIT INTL), (49,SAFI-ABAD DEZFUL), (49,ABADAN), (48,IMPERIAL CO), (48,K.F.I.A. (KING FAHAD INT. AIRPORT) DAMMA), (48,SIBI), (48,OWATONNA DEGNER RGNL), (48,NEEDLES AIRPORT), (48,AL AHSA), (48,TOUAT CHEIKH SIDI MOHAMED BELKEBIR), (48,TIMIMOUN), (48,NEWCASTLE), (48,ASWAN INTL), (48,DOHA INTL), (48,HASSAKAH), (47,QAISUMAH), (47,KING KHALED MILITARY CIT...\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// results sorting\n",
    "val sortedMaxTempCity = maxTemperaturePerCity\n",
    "    .map({case (k, v) => (v, k)})\n",
    "    .sortByKey(false)\n",
    "    .collect\n",
    "\n",
    "val sortedMaxTempCityUK = maxTemperaturePerCityInUK\n",
    "    .map({case (k, v) => (v, k)})\n",
    "    .sortByKey(false)\n",
    "    .collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c47156d-62bd-42cf-bb15-5d2496f8b882",
   "metadata": {
    "id": "0c47156d-62bd-42cf-bb15-5d2496f8b882"
   },
   "source": [
    "## 103-4 Memory occupation\n",
    "\n",
    "Use Spark's web UI to verify the space occupied by the provided RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af3068b3-f2aa-4d13-812b-7d0461a35390",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:12:02.380987Z",
     "start_time": "2024-10-20T16:12:02.234579Z"
    },
    "id": "af3068b3-f2aa-4d13-812b-7d0461a35390",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.storage.StorageLevel._\n",
       "memRdd: rddWeather.type = MapPartitionsRDD[2] at map at <console>:30\n",
       "memSerRdd: org.apache.spark.rdd.RDD[(String, String, String, String, String, Int, Boolean)] = MapPartitionsRDD[45] at map at <console>:39\n",
       "diskRdd: org.apache.spark.rdd.RDD[(String, String, String, String, String, Int, Boolean)] = MapPartitionsRDD[46] at map at <console>:40\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.storage.StorageLevel._\n",
    "\n",
    "sc.getPersistentRDDs.foreach(_._2.unpersist())\n",
    "\n",
    "val memRdd = rddWeather.cache()\n",
    "val memSerRdd = memRdd.map(x=>x).persist(MEMORY_ONLY_SER)\n",
    "val diskRdd = memRdd.map(x=>x).persist(DISK_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a31c2023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: (String, String, String, String, String, Int, Boolean) = (010010,99999,2000,01,01,0,true)\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memRdd.first\n",
    "memSerRdd.first\n",
    "diskRdd.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fbf56bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: Long = 4987830\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memRdd.count\n",
    "memSerRdd.count\n",
    "diskRdd.count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c7bc50-bb59-4e70-8955-8a44d7de774d",
   "metadata": {
    "id": "f4c7bc50-bb59-4e70-8955-8a44d7de774d"
   },
   "source": [
    "## 103-5 Evaluating different join methods\n",
    "\n",
    "Consider the following scenario:\n",
    "- We have a disposable RDD of Weather data (i.e., it is used only once): ```rddW```\n",
    "- And we have an RDD of Station data that is used many times: ```rddS```\n",
    "- Both RDDs are cached (```collect()```is called to enforce caching)\n",
    "\n",
    "We want to join the two RDDS. Which option is best?\n",
    "- Simply join the two RDDs\n",
    "- Enforce on ```rddW1``` the same partitioner of ```rddS``` (and then join)\n",
    "- Exploit broadcast variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d77122-8bdd-4784-a86e-f42f2da06759",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:18:53.686421Z",
     "start_time": "2024-10-20T16:18:51.290892Z"
    },
    "id": "31d77122-8bdd-4784-a86e-f42f2da06759",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "val p = new HashPartitioner(8)\n",
    "sc.getPersistentRDDs.foreach(_._2.unpersist())\n",
    "\n",
    "val rddW = rddWeather.\n",
    "  filter(_._6<999).\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  persist()\n",
    "val rddS = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  partitionBy(p).\n",
    "  cache()\n",
    "\n",
    "// Collect to enforce caching\n",
    "rddW.collect()\n",
    "rddS.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a6822816cd65d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:19:05.603687Z",
     "start_time": "2024-10-20T16:19:04.730871Z"
    }
   },
   "outputs": [],
   "source": [
    "// Is it better to simply join the two RDDs..\n",
    "rddW.\n",
    "  join(rddS).\n",
    "  map({case(k,v)=>(v._2._3,v._1._6)}).\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0e5f9827be45d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:19:26.072648Z",
     "start_time": "2024-10-20T16:19:25.244226Z"
    }
   },
   "outputs": [],
   "source": [
    "// ..to enforce on rddW1 the same partitioner of rddS..\n",
    "rddW.\n",
    "  partitionBy(p).\n",
    "  join(rddS).\n",
    "  map({case(k,v)=>(v._2._3,v._1._6)}).\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50b618652ac67fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:19:36.264001Z",
     "start_time": "2024-10-20T16:19:35.988801Z"
    }
   },
   "outputs": [],
   "source": [
    "// ..or to exploit broadcast variables?\n",
    "val bRddS = sc.broadcast(rddS.map(x => (x._1, x._2._3)).collectAsMap())\n",
    "val rddJ = rddW.\n",
    "  map({case (k,v) => (bRddS.value.get(k),v._6)}).\n",
    "  filter(_._1!=None)\n",
    "rddJ.\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cc81c0-1425-4ef9-8a19-a7edca031c33",
   "metadata": {
    "id": "e9cc81c0-1425-4ef9-8a19-a7edca031c33"
   },
   "source": [
    "## 103-6 Optimizing Exercise 3\n",
    "\n",
    "Start from the result of the last job of Exercise 3; is there a more efficient way to compute the same result?\n",
    "- Try it on weather-sample10\n",
    "- Hint: consider that each station is located in only one country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47748353-fb4b-432f-af79-d1136453b956",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:39:53.616157Z",
     "start_time": "2024-10-20T16:39:17.055562Z"
    },
    "id": "47748353-fb4b-432f-af79-d1136453b956",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "import org.apache.spark.storage.StorageLevel._\n",
    "val p = new HashPartitioner(8)\n",
    "sc.getPersistentRDDs.foreach(_._2.unpersist())\n",
    "\n",
    "val rddS = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  partitionBy(p).\n",
    "  cache()\n",
    "val rddW = rddWeather.\n",
    "  filter(_._6<999).\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  partitionBy(p).\n",
    "  persist(MEMORY_AND_DISK_SER)\n",
    "\n",
    "// Collect to enforce caching\n",
    "rddW.collect()\n",
    "rddS.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f448cc-efc7-4793-a3a2-4a19e0e6fc15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:29:51.299610Z",
     "start_time": "2024-10-20T16:29:50.682782Z"
    },
    "id": "67f448cc-efc7-4793-a3a2-4a19e0e6fc15",
    "tags": []
   },
   "outputs": [],
   "source": [
    "// First version\n",
    "rddW.\n",
    "  join(rddS).\n",
    "  filter(_._2._2._4==\"UK\").\n",
    "  map({case(k,v)=>(v._2._3,v._1._6)}).\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  map({case(k,v)=>(v,k)}).\n",
    "  sortByKey(false).\n",
    "  collect()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "302-solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
